{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import required libraries\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\n\nimport json\nfrom pathlib import Path\nfrom types import SimpleNamespace","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating datasets from tfrecords\nTRAIN_DIR = \"/kaggle/input/tfrecords20k4/tfrecords20k4\"\nVALIDATION_DIR = \"/kaggle/input/tfrecords-val-new/tfrecords20kvalnew\"\n\ntrain_builder = tfds.builder_from_directory(TRAIN_DIR)\nvalidation_builder = tfds.builder_from_directory(VALIDATION_DIR)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting an image to tensor\nimage = tf.keras.preprocessing.image.load_img(path)\nimg_tensor = tf.keras.preprocessing.image.img_to_array(image, dtype=\"uint8\")\nimg_tensor = tf.image.resize(img_tensor, (224, 224))\ntf.cast(img_tensor, tf.uint8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading metadata file\nmetadata = pd.read_csv(path, sep=\"\\t\")\nnew_metadata = metadata.copy()\n# new_metadata[\"dataset_name\"] = /.name\n# return new_metadata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting all Categories\ncategories = metadata[\"Category\"].unique().tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting all ingredients\nunique_ingredients = set()\nfor ingredient_list in metadata[\"Ingredients\"]:\n    ingredient_list = ingredient_list.split(\",\")\n    unique_ingredients.update(ingredient_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate tensors for the images\ndef get_tensors(self, index):\n        img_tensor = load_image_to_arr(img_path)\n        if img_path.suffix == \".jpeg\" or img_path.suffix == \".jpg\":\n            img_tensor = tf.io.encode_jpeg(img_tensor, format=\"rgb\")\n        elif img_path.suffix == \".png\":\n            img_tensor = tf.io.encode_png(img_tensor)\n           \n        calorie_tensor = row[\"Calorie(kcal)\"]\n        carbs_tensor = row[\"Carbohydrate(g)\"]\n        protein_tensor = row[\"Protein(g)\"]\n        fat_tensor = row[\"Fat(g)\"]\n        return img_tensor, {\n            \"category_output\": tf.constant(row[\"Category\"]),\n            \"calorie_output\": tf.constant(calorie_tensor),\n            \"carbs_output\": tf.constant(carbs_tensor),\n            \"protein_output\": tf.constant(protein_tensor),\n            \"fat_output\": tf.constant(fat_tensor),\n            \"ingredients_output\": tf.constant(row[\"Ingredients\"]),\n        }\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flatten_tensors(self, tensor):\n        result = []\n        img_data = tensor[0].numpy()\n        others_data = [value.numpy() for key, value in tensor[1].items()]\n        result.append(img_data)\n        result.extend(others_data)\n        return result","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_file_data(index, dataset_index):\n    target_dataset = DATASETS[dataset_index]\n    return target_dataset.flatten_tensors(target_dataset.get_tensors(index))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_data_pipeline(datasets, sample_size=None):\n    if sample_size is None:\n        sample_size = [1.0] * len(datasets)\n    assert len(sample_size) == len(\n        datasets\n    ), \"Illegal array of sample sizes provided. Number of sample size does not match number of datasets\"\n    file_pointers = [\n        x.extract_file_pointers().sample(frac=s, random_state=999)\n        for x, s in zip(datasets, sample_size)\n    ]\n    all_file_pointers = pd.concat(file_pointers).sample(frac=1, random_state=999)\n    print(f\"Total samples : {len(all_file_pointers)}\")\n    \n    all_file_pointers[\"dataset_name\"] = all_file_pointers[\"dataset_name\"].apply(\n        lambda x: DATASETS_NAME.index(x)\n    )\n\n    final_dataset = tf.data.Dataset.from_tensor_slices(\n        (\n            all_file_pointers[\"metadata_index\"].tolist(),\n            all_file_pointers[\"dataset_name\"].tolist(),\n        )\n    )\n    return final_dataset","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def get_category_one_hot_encoding(self, category_name):\n        index = self.all_food_categories_integer_encoded[category_name]\n        assert index is not None, f\"{category_name} does not have an integer mapping\"\n        num_classes = len(self.all_food_categories)\n        return keras.utils.to_categorical(index, num_classes, dtype=\"uint8\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ingredients_one_hot_encoding(self, ingredient_list):\n        ingredient_list = list(\n            map(lambda x: self.__transform_ingredient_to_integer(x), ingredient_list)\n        )\n        multi_one_hot_layer = tf.keras.layers.CategoryEncoding(\n            num_tokens=len(self.all_ingredients), output_mode=\"multi_hot\"\n        )\n        return tf.cast(multi_one_hot_layer(ingredient_list), dtype=tf.uint8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def __transform_ingredient_to_integer(self, ingredient_name):\n        index = self.all_ingredients_integer_encoded[ingredient_name]\n        assert index is not None, f\"{ingredient_name} does not have an integer mapping\"\n        return index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def __encode_categories_to_integers(self):\n        return {\n            category_name: index\n            for index, category_name in enumerate(self.all_food_categories)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def __encode_ingredients_to_integers(self):\n        return {\n            ingredient_name: index\n            for index, ingredient_name in enumerate(self.all_ingredients)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nFOOD101 = Food101(\n    image_dir=\"data/images\",\n    metadata_dir=\"data/metadata\",\n)\n\nDATASETS = [FOOD101]\nDATASETS_NAME = [x.name for x in DATASETS]\n\ndef create_one_hot_encoder(datasets):\n    all_categories = []\n    all_ingredients = []\n    for x in datasets:\n        all_categories.extend(x.all_categories)\n        all_ingredients.extend(x.all_ingredients)\n    all_categories = set(all_categories)\n    all_ingredients = set(all_ingredients)\n    return OneHotEncoder([*all_categories], [*all_ingredients])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(\n    train_dataset_train,\n    validation_dataset_train,\n) = food101_builder.as_dataset(split=[\"train[:70%]\", \"train[70%:]\"])\n\ntrain_dataset = train_dataset_train.concatenate(validation_dataset_train)\n\n(\n    train_dataset_val,\n    validation_dataset_val,\n) = food101_val_builder.as_dataset(split=[\"train[:70%]\", \"train[70%:]\"])\n\nvalidation_dataset = train_dataset_val.concatenate(validation_dataset_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total validation size : {train_dataset.cardinality().numpy()}\")\nprint(f\"Total validation size : {validation_dataset.cardinality().numpy()}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_dataset = (\n    train_dataset.map(parse_function).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n)\nvalidation_dataset = (\n    validation_dataset.map(parse_function).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compile_model(self, optimizer):\n        assert self.model is not None\n        category_classification_loss = keras.losses.CategoricalCrossentropy()\n        calorie_regression_loss = keras.losses.MeanAbsoluteError()\n        carbs_regression_loss = keras.losses.MeanAbsoluteError()\n        protein_regression_loss = keras.losses.MeanAbsoluteError()\n        fat_regression_loss = keras.losses.MeanAbsoluteError()\n        ingredient_multilabel_loss = keras.losses.BinaryCrossentropy()\n        category_classification_metrics = (\n            [\n                keras.metrics.CategoricalAccuracy(name=\"acc\"),\n                keras.metrics.Precision(name=\"precision\"),\n                keras.metrics.Recall(name=\"recall\"),\n            ],\n        )\n\n        calorie_regression_metrics = [keras.metrics.MeanAbsoluteError(name=\"MAE\")]\n        carbs_regression_metrics = [keras.metrics.MeanAbsoluteError(name=\"MAE\")]\n        protein_regression_metrics = [keras.metrics.MeanAbsoluteError(name=\"MAE\")]\n        fat_regression_metrics = [keras.metrics.MeanAbsoluteError(name=\"MAE\")]\n        ingredient_multilabel_metrics = (\n            [\n                keras.metrics.Precision(name=\"precision\"),\n                keras.metrics.Recall(name=\"recall\"),\n            ],\n        )\n        category_classification_loss_weights = 1.0\n        ingredient_multilabel_loss_weights = 1.0\n        calorie_regression_loss_weights = 1.0\n        carbs_regression_loss_weights = 1.0\n        protein_regression_loss_weights = 1.0\n        fat_regression_loss_weights = 1.0\n\n        self.model.compile(\n            optimizer=optimizer,\n            loss={\n                \"category_output\": category_classification_loss,\n                \"calorie_output\": calorie_regression_loss,\n                \"carbs_output\": carbs_regression_loss,\n                \"protein_output\": protein_regression_loss,\n                \"fat_output\": fat_regression_loss,\n                \"ingredients_output\": ingredient_multilabel_loss,\n            },\n            metrics={\n                \"category_output\": category_classification_metrics,\n                \"calorie_output\": calorie_regression_metrics,\n                \"carbs_output\": carbs_regression_metrics,\n                \"protein_output\": protein_regression_metrics,\n                \"fat_output\": fat_regression_metrics,\n                \"ingredients_output\": ingredient_multilabel_metrics,\n            },\n            loss_weights={\n                \"category_output\": category_classification_loss_weights,\n                \"calorie_output\": calorie_regression_loss_weights,\n                \"carbs_output\": carbs_regression_loss_weights,\n                \"protein_output\": protein_regression_loss_weights,\n                \"fat_output\": fat_regression_loss_weights,\n                \"ingredients_output\": ingredient_multilabel_loss_weights,\n            },\n        )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze_category_classification_layers(self):\n    assert self.model is not None\n    submodel = self.model.get_layer(\"category_output\")\n    submodel.trainable = False\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def freeze_convolution_base(self):\n        assert self.model is not None\n        submodel = self.model.get_layer(\"efficientnetB1\")\n        submodel.trainable = False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_callbacks(self):\n        tensorboard_dir = f\"./models/logs/{self.name}/{self.model_config_name}\"\n        checkpoint_dir = f\"./temp/checkpoint/{self.name}/{self.model_config_name}\"\n        checkpoint_model_path = checkpoint_dir + \"/model\"\n        checkpoint_weights_path = checkpoint_dir + \"/weights/ckpt\"\n        checkpoint_model_callback = keras.callbacks.ModelCheckpoint(\n            checkpoint_model_path, monitor=\"val_loss\", save_best_only=True, mode=\"min\"\n        )\n        checkpoint_weights_callback = keras.callbacks.ModelCheckpoint(\n            checkpoint_weights_path,\n            monitor=\"val_loss\",\n            save_best_only=True,\n            save_weights_only=True,\n            mode=\"min\",\n        )\n        reduce_lr_callback = keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-6, verbose=1\n        )\n        tensorboard_callback = keras.callbacks.TensorBoard(log_dir=tensorboard_dir)\n        early_stopping_callback = keras.callbacks.EarlyStopping(\"val_loss\", patience=8)\n        return [\n            tensorboard_callback,\n            early_stopping_callback,\n            checkpoint_model_callback,\n            checkpoint_weights_callback,\n            reduce_lr_callback,\n        ]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Layers from bottom to top (classifier)  #\n    keras.Input(shape=self.input_shape)\n\n    input_layer = keras.layers.Input(shape=self.input_shape)\n    augmentation_layer = keras.layers.RandomFlip()(input_layer)\n    augmentation_layer = keras.layers.RandomRotation(0.2)(augmentation_layer)\n    keras.Model(inputs=input_layer, outputs=augmentation_layer, name=\"augmentation_layers\")\n\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        shared_layer = keras.layers.Flatten()(input_layer)\n        shared_layer = keras.layers.Dense(\n            num_units[0], activation=\"relu\", name=\"shared_dense_1\"\n        )(shared_layer)\n        shared_layer = keras.layers.BatchNormalization()(shared_layer)\n        output_layer = keras.layers.Dropout(0.2)(shared_layer)\n        return keras.Model(\n            inputs=input_layer, outputs=output_layer, name=\"shared_layers\"\n        )\n\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        x = keras.layers.Dense(\n            num_units[0], activation=\"relu\", name=\"category_dense_1\"\n        )(input_layer)\n        x = keras.layers.BatchNormalization()(x)\n        category_classification_layer = keras.layers.Dense(\n            total_categories, activation=\"softmax\", name=\"category_output_layer\"\n        )(x)\n        output_model = keras.Model(\n            inputs=input_layer,\n            outputs=category_classification_layer,\n            name=\"category_output\",\n        )\n    \n#     calorie layers\n    input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        x = keras.layers.Dense(num_units[0], activation=\"relu\", name=\"calorie_dense_1\")(\n            input_layer\n        )\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dense(num_units[1], activation=\"relu\", name=\"calorie_dense_2\")(\n            x\n        )\n        x = keras.layers.BatchNormalization()(x)\n        calorie_regression_layers = keras.layers.Dense(1, name=\"calorie_output_layer\")(\n            x\n        )\n        return keras.Model(\n            inputs=input_layer, outputs=calorie_regression_layers, name=\"calorie_output\"\n        )\n\n#     carbs layrers\n#     def get_carbs_regression_layers(self, input_tensor, *num_units):\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        x = keras.layers.Dense(num_units[0], activation=\"relu\", name=\"carbs_dense_1\")(\n            input_layer\n        )\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dense(num_units[1], activation=\"relu\", name=\"carbs_dense_2\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        carbs_regression_layers = keras.layers.Dense(1, name=\"carbs_output_layer\")(x)\n        return keras.Model(\n            inputs=input_layer, outputs=carbs_regression_layers, name=\"carbs_output\"\n        )\n\n#     protein layers\n#     def get_protein_regression_layers(self, input_tensor, *num_units):\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        x = keras.layers.Dense(num_units[0], activation=\"relu\", name=\"protein_dense_1\")(\n            input_layer\n        )\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dense(num_units[1], activation=\"relu\", name=\"protein_dense_2\")(\n            x\n        )\n        x = keras.layers.BatchNormalization()(x)\n        protein_regression_layers = keras.layers.Dense(1, name=\"protein_output_layer\")(\n            x\n        )\n        return keras.Model(\n            inputs=input_layer, outputs=protein_regression_layers, name=\"protein_output\"\n        )\n\n    \n# fat layers\n#     def get_fat_regression_layers(self, input_tensor, *num_units):\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        x = keras.layers.Dense(num_units[0], activation=\"relu\", name=\"fat_dense_1\")(\n            input_layer\n        )\n        x = keras.layers.BatchNormalization()(x)\n        x = keras.layers.Dense(num_units[1], activation=\"relu\", name=\"fat_dense_2\")(x)\n        x = keras.layers.BatchNormalization()(x)\n        fat_regression_layers = keras.layers.Dense(1, name=\"fat_output_layer\")(x)\n        return keras.Model(\n            inputs=input_layer, outputs=fat_regression_layers, name=\"fat_output\"\n        )\n\n#     ingre\n#     def get_ingredients_multilabel_layers(\n#         self, input_tensor, total_ingredients, *num_units\n#     ):\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        ingredients_multilabel_layers = keras.layers.Dense(\n            num_units[0], activation=\"relu\", name=\"ingredients_dense_1\"\n        )(input_layer)\n        ingredients_multilabel_layers = keras.layers.BatchNormalization()(\n            ingredients_multilabel_layers\n        )\n        ingredients_multilabel_layers = keras.layers.Dense(\n            num_units[1], activation=\"relu\", name=\"ingredients_dense_2\"\n        )(ingredients_multilabel_layers)\n        ingredients_multilabel_layers = keras.layers.BatchNormalization()(\n            ingredients_multilabel_layers\n        )\n        ingredients_multilabel_layers = keras.layers.Dense(\n            num_units[2], activation=\"relu\", name=\"ingredients_dense_3\"\n        )(ingredients_multilabel_layers)\n        ingredients_multilabel_layers = keras.layers.BatchNormalization()(\n            ingredients_multilabel_layers\n        )\n        output_layers = keras.layers.Dense(\n            total_ingredients, activation=\"sigmoid\", name=\"ingredients_output_layer\"\n        )(ingredients_multilabel_layers)\n        return keras.Model(\n            inputs=input_layer, outputs=output_layers, name=\"ingredients_output\"\n        )\n\n    # End of layers  #\n\n    def load_model(self, path):\n        self.model = keras.models.load_model(path)\n        \n    def evaluate(self, validation_dataset):\n        assert self.model is not None\n        self.model.evaluate(validation_dataset)  \n    \n    def predict(self, validation_dataset):\n        assert self.model is not None\n        self.model.predict(validation_dataset)\n\n    def print_summary(self):\n        assert (\n            self.model is not None\n        ), \"Please run build_and_compile before printing summary.\"\n        self.model.summary(expand_nested=True, show_trainable=True)\n\n    def save_model(self, path):\n        assert self.model is not None\n        self.model.save(path, save_format=\"h5\")\n\n    def train_model(self, **kwargs):\n        assert self.model is not None, \"No model found.\"\n        return self.model.fit(**kwargs, callbacks=self.get_callbacks())\n\n    def unfreeze_category_classification_layers(self):\n        assert self.model is not None\n        submodel = self.model.get_layer(\"category_output\")\n        submodel.trainable = False\n\n    def unfreeze_convolution_base(self, fine_tune_at=0):\n        assert self.model is not None\n        submodel = self.model.get_layer(\"efficientnetb1\")\n        submodel.trainable = True\n        for layer in submodel.layers[:fine_tune_at]:\n            layer.trainable = False\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlatModel(BaseModel):\n        model_inputs = self.get_input_layer()\n        prev_layer = self.get_augmentation_layers()(model_inputs)\n        prev_layer = self.get_preprocess_layers(prev_layer)(prev_layer)\n        prev_layer = self.get_convolution_block()(prev_layer)\n        prev_layer = self.get_shared_layers(prev_layer, *shared_units)(prev_layer)\n        category_classification_head = self.get_category_classification_layers(\n            prev_layer, self.total_food_category, *independent_category_units\n        )(prev_layer)\n        ingredients_multilabel_head = self.get_ingredients_multilabel_layers(\n            prev_layer, self.total_ingredients_category, *independent_ingredients_units\n        )(prev_layer)\n        calorie_regression_head = self.get_calorie_regression_layers(\n            prev_layer, *independent_calorie_units\n        )(prev_layer)\n        carbs_regression_head = self.get_carbs_regression_layers(\n            prev_layer, *independent_carbs_units\n        )(prev_layer)\n        protein_regression_head = self.get_protein_regression_layers(\n            prev_layer, *independent_protein_units\n        )(prev_layer)\n        fat_regression_head = self.get_fat_regression_layers(\n            prev_layer, *independent_fat_units\n        )(prev_layer)\n\n        model = keras.Model(\n            inputs=model_inputs,\n            outputs=[\n                category_classification_head,\n                ingredients_multilabel_head,\n                calorie_regression_head,\n                carbs_regression_head,\n                protein_regression_head,\n                fat_regression_head,\n            ],\n            name=self.name,\n        )\n        self.model = model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlatEfficientNetB1Model(FlatModel):\n    def get_preprocess_layers(self, input_tensor):\n        input_layer = keras.layers.Input(shape=input_tensor.shape[1:])\n        output_layer = keras.applications.efficientnet.preprocess_input(input_layer)\n        return keras.Model(\n            inputs=input_layer, outputs=output_layer, name=\"preprocessing_layers\"\n        )\n\n    def get_convolution_block(self):\n        efficientnet_convolution_layers = (\n            keras.applications.efficientnet.EfficientNetB1(\n                input_shape=self.input_shape,\n                include_top=False,\n                weights=\"imagenet\",\n                pooling=\"avg\",\n            )\n        )\n        efficientnet_convolution_layers.trainable = False\n        return efficientnet_convolution_layers\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"flat_efficientnet = FlatEfficientNetB1Model(\n    input_shape=(224, 224, 3),\n    total_food_category=len(EXPORTED.one_hot_encoder.all_food_categories),\n    total_ingredients_category=len(EXPORTED.one_hot_encoder.all_ingredients),\n    model_config_name=\"finalIngredients\",\n)\n\nflat_efficientnet.build(\n    shared_units=[2048],\n    independent_category_units=[512],\n    independent_ingredients_units=[1024, 512, 256],\n    independent_protein_units=[64, 32],\n    independent_fat_units=[64, 32],\n    independent_carbs_units=[64, 32],\n    independent_calorie_units=[64, 32],\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x=train_dataset, validation_data=validation_dataset, epochs=30, verbose=2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(validation_dataset)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/model_100_epcohs', save_format=\"h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = tf.keras.preprocessing.image.load_img('/kaggle/input/pizza-test/1001116.jpg')\nimg_tensor = tf.keras.preprocessing.image.img_to_array(image, dtype=\"uint8\")\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor = tf.image.resize(img_tensor, (224, 224))\ntf.cast(img_tensor, tf.uint8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_output, ing_output, cal_output, carbs_output, pro_output, fat_output = model.predict(img_tensor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = category_output[0]\ncat_pred = np.where(n == np.max(n))[0][0]\nprint(cat_pred)\n# 1 prediction -> 98 categories\n\nfor key, value in EXPORTED.one_hot_encoder.all_food_categories_integer_encoded.items():\n    if value == cat_pred:\n        print(\"Category:\", key)\n\ni = ing_output[0]\nl = np.where(i >= 0.5)\nfor key, value in EXPORTED.one_hot_encoder.all_ingredients_integer_encoded.items():\n    for ing_pred in l[0]:\n        if value == ing_pred:\n            print(\"Ingredients\", key)\n# prediction -> 871 ingredients\n\nprint(\"Calories:\", cal_output[0])\n# 1 prediction -> 1 cal\n\nprint(\"Carbs:\", carbs_output[0])\n# 1 prediction -> 1 carbs\n\nprint(\"Proteins:\", pro_output[0])\n# 1 prediction -> 1 protein\n\nprint(\"Fat:\", fat_output[0])\n# 1 prediction -> 1 fat","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converting .h5 model to .json\n!tensorflowjs_converter --input_format=keras '/kaggle/input/model-100-epochs/model_100_epcohs' '/kaggle/working/'","metadata":{},"execution_count":null,"outputs":[]}]}